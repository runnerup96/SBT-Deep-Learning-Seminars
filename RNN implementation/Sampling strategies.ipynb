{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fafafd7-94e7-4e24-8fa0-79ba92b0334f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48dec6f9-f41d-4c1b-8b04-5965a89b8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0b2393-3e8a-4336-8f6a-bf1e396db82a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b9e670c5b4472eb3b6ecdb4b3c481c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db91b3ae0ead43d894ab44161049bb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468732a86f7b49c2bb0b0bb3cd0a5552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5325432624104458858fcaa89e5d9b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175eb9c54328475db82e1b7bd5f680df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b2816f1fa54647ba01517ef858de80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2161683340e48b199be97cfb647fb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9c9283-cdb9-4f14-9e39-896a328557be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=1.0, top_k=0, top_p=1.0):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7a33a-1286-4409-9ed2-582a0ffee0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64baf9f6-0304-4529-b9e1-5faccd846006",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdfbb93f-ac4b-4573-b158-e0a60b9fd68c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature: 0.2 ---\n",
      "The future of AI is not yet clear.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "The future of AI is the culmination of a long and long, long process of rationalizing the laws, and of instituting a new and better-informed society.”\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "The future of AI is no easy one to reach, ourselves at most, with some serious problems, but I doubt others will. Suddenly Swarm brings us along. Even if it does not be a decade ago we will still be accompanied by intelligent intelligent AI\n",
      "\n",
      "--- Temperature: 1.5 ---\n",
      "The future of AI is perfectly familiar since User monopoly technology came Westite from signaling collaboration to federal government power players Lavern snippité wave! Thank you!Xavier Contatz himself and Mozilla SUPULAR XCC name. Goair Luckisonbear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.2, 0.7, 1.0, 1.5]:\n",
    "    print(f\"--- Temperature: {temp} ---\")\n",
    "    print(generate_text(prompt, temperature=temp, top_k=0, top_p=1.0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72277e2d-ebb6-4022-be47-e4ba5a4259d8",
   "metadata": {},
   "source": [
    "### Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44460185-c20d-4541-b90a-d8634267e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-k: 0 ---\n",
      "The future of AI is just a question of what happens if it enforces individual preferences. For instance, even a community like Armin will join a group of researchers who are interested in enabling users to engage in social networks. The idea with open ecosystem\n",
      "\n",
      "--- Top-k: 10 ---\n",
      "The future of AI is to be a lot more interesting. I'm looking forward to seeing how AI can make things work.\n",
      "\n",
      "--- Top-k: 50 ---\n",
      "The future of AI is one of human decisions. We need to do this with compassion. This can come from thinking about AI on a personal level. One of our biggest challenges is that while we know that we have something that can be learned and can\n",
      "\n",
      "--- Top-k: 100 ---\n",
      "The future of AI is set in motion as the next phase of exploration and exploitation of the world's wonders is underway.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The new world of AI is up for grabs by the human race on a global scale, from Earth to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [0, 10, 50, 100]:\n",
    "    print(f\"--- Top-k: {k} ---\")\n",
    "    print(generate_text(prompt, temperature=1.0, top_k=k, top_p=1.0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195dd79-e319-44db-8f8b-ebbdb745a9b3",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c624e5-76ef-41df-b127-ad9a19a3964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-p: 1.0 ---\n",
      "The future of AI is a very uncertain one, even although today the company is working hard to develop and understand how they do it and it's a very important topic. At Google, as it has always been on paper following Google, 80% of\n",
      "\n",
      "--- Top-p: 0.9 ---\n",
      "The future of AI is still alive and well. We have many ideas, but only a few to share. I've given myself one, at least so far, about how our AI could be so similar to human. There is some sense in both\n",
      "\n",
      "--- Top-p: 0.8 ---\n",
      "The future of AI is not so easy, but the level of knowledge gained by AI from certain human factors can take time, both in terms of helping humans learn and to give them the skills they need to succeed, and for those they have, there\n",
      "\n",
      "--- Top-p: 0.5 ---\n",
      "The future of AI is not clear.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [1.0, 0.9, 0.8, 0.5]:\n",
    "    print(f\"--- Top-p: {p} ---\")\n",
    "    print(generate_text(prompt, temperature=1.0, top_k=0, top_p=p))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf606a-e4c5-4d16-88bd-4e3d08c3c6f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59072e6c-e92f-4f96-b4c8-632ba7966618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba31afe-0397-42a2-87e2-c39712a03429",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4f201e7-6794-4eac-afae-d9a34d5a6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai(prompt, temperature=1.0, top_p=1.0, model=\"gpt-4o\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=40\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843567a4-2e70-4c41-b525-2c4f0f968ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78fbb3-3058-4c6b-bc2e-325665bdfb09",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26869f0a-93f2-4a79-895d-e89774f0c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.2 ---\n",
      "The future of AI is a topic of great interest and speculation, with numerous potential developments and implications across various fields. Here are some key areas where AI is expected to have a significant impact:\n",
      "\n",
      "1.\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "The future of AI is poised to be transformative across numerous sectors and aspects of daily life. Here are some potential directions and trends we might expect:\n",
      "\n",
      "1. **Advanced Automation**: AI will continue to\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "The future of AI is a topic of much speculation and excitement, with a range of potential developments and implications spanning various fields. Here are a few key areas where AI is expected to have a significant impact\n",
      "\n",
      "--- Temperature: 1.5 ---\n",
      "a topic full of promise and speculation. As AI technology continues to evolve, there are several likely trends and potential developments:\n",
      "\n",
      "1. **Ubiquitous Integration**: AI will become more seamlessly integrated\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.2, 0.7, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Temperature: {temp} ---\")\n",
    "    print(generate_openai(prompt, temperature=temp, top_p=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba1973-5979-4854-9bd9-4f7143a0b5af",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52b2d021-b993-4073-b0a4-c4a67cdc178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top-p: 1.0 ---\n",
      "a highly debated and dynamic topic with numerous possibilities and challenges. Several key areas are likely to shape the future of AI:\n",
      "\n",
      "1. **Advancements in Machine Learning**: Continued improvements in algorithms, increased\n",
      "\n",
      "--- Top-p: 0.9 ---\n",
      "The future of AI is poised to be transformative and multifaceted, impacting various aspects of society, technology, and industry. Here are some key trends and potential developments:\n",
      "\n",
      "1. **Advancements in Machine\n",
      "\n",
      "--- Top-p: 0.8 ---\n",
      "The future of AI is a topic of considerable speculation and debate, with many potential directions and outcomes. Here are some key areas where AI is expected to have a significant impact:\n",
      "\n",
      "1. **Healthcare**\n",
      "\n",
      "--- Top-p: 0.5 ---\n",
      "The future of AI is a topic of much speculation and excitement, as it holds the potential to transform various aspects of society, technology, and industry. Here are some key areas where AI is expected to\n"
     ]
    }
   ],
   "source": [
    "for p in [1.0, 0.9, 0.8, 0.5]:\n",
    "    print(f\"\\n--- Top-p: {p} ---\")\n",
    "    print(generate_openai(prompt, temperature=1.0, top_p=p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
